{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from utils.data_utils import (DynamicBatchSampler, collate_fn, get_dataset,\n",
    "                              get_grouping, get_static_dataset,\n",
    "                              get_test_dataset, get_test_synthetic_dataset)\n",
    "from utils.optuna_utils import load_best_model\n",
    "from utils.result_utils import (inference, plot_3d_combined_pdfs, plot_pdf,\n",
    "                                report_results)\n",
    "from utils.train_utils import ComparisonQuantileLoss, TwoStageQuantileLoss, train\n",
    "from LinearRegression import LinearQuantileRegression\n",
    "\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "quantiles = config[\"general\"][\"quantiles\"]\n",
    "validation_start_date = config[\"general\"][\"dates\"][\"validation_period\"][\"start_date\"]\n",
    "validation_end_date = config[\"general\"][\"dates\"][\"validation_period\"][\"end_date\"]\n",
    "test_start_date = config[\"general\"][\"dates\"][\"test_period\"][\"start_date\"]\n",
    "test_end_date = config[\"general\"][\"dates\"][\"test_period\"][\"end_date\"]\n",
    "loss_fn = TwoStageQuantileLoss(quantiles)\n",
    "test_loss_fn = ComparisonQuantileLoss(quantiles)\n",
    "results = {}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.makedirs(\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model, lstm_params = load_best_model('lstm')\n",
    "\n",
    "lstm_model.to(DEVICE)\n",
    "\n",
    "lstm_normalization_window = lstm_params['normalazation_window']\n",
    "lstm_batch_size = lstm_params['batch_size']\n",
    "l1_reg = lstm_params['l1_reg']\n",
    "l2_reg = lstm_params['l2_reg']\n",
    "\n",
    "lstm_optimizer = torch.optim.Adam(lstm_model.parameters(\n",
    "), lr=lstm_params['learning_rate'], weight_decay=l2_reg)\n",
    "\n",
    "print(lstm_model)\n",
    "print(f\"Model has {sum(p.numel() for p in lstm_model.parameters() if p.requires_grad)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('lstm_model.pth'):\n",
    "    lstm_model.load_state_dict(torch.load('lstm_model.pth'))\n",
    "    lstm_model.to(DEVICE)\n",
    "    print(\"Model loaded from lstm_model.pth\")\n",
    "else:\n",
    "    lstm_train_dataset = get_dataset(\n",
    "        lstm_normalization_window, \"1998-01-01\", validation_start_date)\n",
    "    lstm_val_dataset = get_dataset(lstm_normalization_window,\n",
    "                                   validation_start_date, validation_end_date)\n",
    "    lstm_train_batch_sampler = DynamicBatchSampler(\n",
    "        lstm_train_dataset, batch_size=lstm_batch_size)\n",
    "    lstm_val_batch_sampler = DynamicBatchSampler(lstm_val_dataset, batch_size=lstm_batch_size)\n",
    "\n",
    "    lstm_train_loader = DataLoader(\n",
    "        lstm_train_dataset, batch_sampler=lstm_train_batch_sampler, collate_fn=collate_fn)\n",
    "    lstm_val_loader = DataLoader(\n",
    "        lstm_val_dataset, batch_sampler=lstm_val_batch_sampler, collate_fn=collate_fn)\n",
    "    _, lstm_model = train(\n",
    "        model=lstm_model,\n",
    "        train_loader=lstm_train_loader,\n",
    "        val_loader=lstm_val_loader,\n",
    "        criterion=loss_fn,\n",
    "        optimizer=lstm_optimizer,\n",
    "        num_epochs=100,\n",
    "        patience=10,\n",
    "        l1_reg=l1_reg,\n",
    "        lstm=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    torch.save(lstm_model.state_dict(), 'lstm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model, dense_params = load_best_model('dense')\n",
    "\n",
    "dense_model.to(DEVICE)\n",
    "\n",
    "dense_normalization_window = dense_params['normalazation_window']\n",
    "dense_batch_size = dense_params['batch_size']\n",
    "l1_reg = dense_params['l1_reg']\n",
    "l2_reg = dense_params['l2_reg']\n",
    "\n",
    "dense_optimizer = torch.optim.Adam(dense_model.parameters(), lr=dense_params['learning_rate'], weight_decay=l2_reg)\n",
    "\n",
    "print(dense_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('dense_model.pth'):\n",
    "    dense_model.load_state_dict(torch.load('dense_model.pth'))\n",
    "    dense_model.to(DEVICE)\n",
    "    print(\"Model loaded from dense_model.pth\")\n",
    "else:\n",
    "    dense_train_dataset = get_static_dataset(dense_normalization_window, \"1998-01-01\", validation_start_date)\n",
    "    dense_val_dataset = get_static_dataset(dense_normalization_window, validation_start_date, validation_end_date)\n",
    "\n",
    "    dense_train_loader = DataLoader(dense_train_dataset, batch_size=dense_batch_size, shuffle=True)\n",
    "    dense_val_loader = DataLoader(dense_val_dataset, batch_size=dense_batch_size, shuffle=False)\n",
    "    _, dense_model = train(\n",
    "        model=dense_model,\n",
    "        train_loader=dense_train_loader,\n",
    "        val_loader=dense_val_loader,\n",
    "        criterion=loss_fn,\n",
    "        optimizer=dense_optimizer,\n",
    "        num_epochs=100,\n",
    "        patience=10,\n",
    "        l1_reg=l1_reg,\n",
    "        verbose=True\n",
    "    )\n",
    "    torch.save(dense_model.state_dict(), 'dense_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"quant_reg.pth\"):\n",
    "    dense_train_dataset = get_static_dataset(dense_normalization_window, \"1998-01-01\", validation_start_date)\n",
    "\n",
    "    big_X = []\n",
    "    big_y = []\n",
    "    for i in range(len(dense_train_dataset)):\n",
    "        x, _, z, y, _ = dense_train_dataset[i]\n",
    "        xx = torch.cat((x, z))\n",
    "        y = y.view(-1).cpu().numpy()\n",
    "        big_X.append(xx.cpu().numpy())\n",
    "        big_y.append(y)\n",
    "    big_X = np.array(big_X)\n",
    "    big_y = np.array(big_y).reshape(-1)\n",
    "\n",
    "    quant_reg = LinearQuantileRegression(quantiles)\n",
    "    quant_reg.train(big_X, big_y)\n",
    "\n",
    "    with open(\"quant_reg.pth\", \"wb\") as f:\n",
    "        pickle.dump(quant_reg, f)\n",
    "else:\n",
    "    with open(\"quant_reg.pth\", \"rb\") as f:\n",
    "        quant_reg = pickle.load(f)\n",
    "    print(\"Model loaded from quant_reg.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_test_dataset = get_test_dataset(dense_normalization_window, test_start_date, test_end_date)\n",
    "lstm_test_dataset = get_test_dataset(lstm_normalization_window, test_start_date, test_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = dense_test_dataset.assets\n",
    "datas = dense_test_dataset.datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "for asset in tqdm(assets, desc='Inferencing models'):\n",
    "    grouping = get_grouping(datas, asset)\n",
    "    if grouping not in results:\n",
    "        results[grouping] = {\n",
    "            'linear': 0,\n",
    "            'dense': 0,\n",
    "            'lstm': 0,\n",
    "        }\n",
    "    dense_test_dataset.set_main_asset(asset)\n",
    "    lstm_test_dataset.set_main_asset(asset)\n",
    "\n",
    "    dense_data_loader = DataLoader(dense_test_dataset, batch_size=1024, shuffle=False)\n",
    "    lstm_data_loader = DataLoader(lstm_test_dataset, batch_size=1024, shuffle=False)\n",
    "    dense_losses = inference(dense_model, dense_data_loader, test_loss_fn, is_dense=True)\n",
    "    lstm_losses = inference(lstm_model, lstm_data_loader, test_loss_fn, is_dense=False)\n",
    "    linear_loss = inference(quant_reg, dense_data_loader, test_loss_fn, is_dense=False,\n",
    "                            is_linear=True)\n",
    "\n",
    "    results[grouping]['linear'] += linear_loss.mean().item()\n",
    "    results[grouping]['dense'] += dense_losses.mean().item()\n",
    "    results[grouping]['lstm'] += lstm_losses.mean().item()\n",
    "results = {k: {model: value / 10 for model, value in v.items()} for k, v in results.items()}\n",
    "market_results = results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_results(market_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_combined_pdfs('NVDA', dense_model, lstm_model, quant_reg,\n",
    "                      dense_test_dataset, lstm_test_dataset, is_linear=False)\n",
    "plot_pdf('NVDA', dense_model, lstm_model, quant_reg,\n",
    "         dense_test_dataset, lstm_test_dataset, is_linear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating Normally Distributed Test Dataset\")\n",
    "dense_normal_test_dataset, lstm_normal_test_dataset = get_test_synthetic_dataset(\n",
    "    dense_normalization_window, lstm_normalization_window, 1000, 0.7, distribution='normal')\n",
    "print(\"Generating Log Normally Distributed Test Dataset\")\n",
    "dense_log_normal_test_dataset, lstm_log_normal_test_dataset = get_test_synthetic_dataset(\n",
    "    dense_normalization_window, lstm_normalization_window, 1000, 0.7, distribution='lognormal')\n",
    "print(\"Generating Gamma Distributed Test Dataset\")\n",
    "dense_gamma_test_dataset, lstm_gamma_test_dataset = get_test_synthetic_dataset(\n",
    "    dense_normalization_window, lstm_normalization_window, 1000, 0.7, distribution='gamma')\n",
    "print(\"Generating Uniform Distributed Test Dataset\")\n",
    "dense_uniform_test_dataset, lstm_uniform_test_dataset = get_test_synthetic_dataset(\n",
    "    dense_normalization_window, lstm_normalization_window, 1000, 0.7, distribution='uniform')\n",
    "\n",
    "synthetic_data = {\n",
    "    'Normal': (dense_normal_test_dataset, lstm_normal_test_dataset),\n",
    "    'Log Normal': (dense_log_normal_test_dataset, lstm_log_normal_test_dataset),\n",
    "    'Gamma': (dense_gamma_test_dataset, lstm_gamma_test_dataset),\n",
    "    'Uniform': (dense_uniform_test_dataset, lstm_uniform_test_dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_results = {}\n",
    "for distribution, (dense_synthetic_test_dataset, lstm_synthetic_test_dataset) in tqdm(synthetic_data.items(), desc='Inferencing synthetic datasets'):\n",
    "    dense_total_loss = 0\n",
    "    lstm_total_loss = 0\n",
    "    for i in range(0, 10):\n",
    "        dense_synthetic_test_dataset.set_main_asset(f\"synthetic_{i}\")\n",
    "        lstm_synthetic_test_dataset.set_main_asset(f\"synthetic_{i}\")\n",
    "        dense_data_loader = DataLoader(dense_synthetic_test_dataset, batch_size=1024, shuffle=False)\n",
    "        lstm_data_loader = DataLoader(lstm_synthetic_test_dataset, batch_size=1024, shuffle=False)\n",
    "        linear_losses = inference(quant_reg, dense_data_loader,\n",
    "                                  test_loss_fn, is_dense=False, is_linear=True)\n",
    "        dense_losses = inference(dense_model, dense_data_loader, test_loss_fn, is_dense=True)\n",
    "        lstm_losses = inference(lstm_model, lstm_data_loader, test_loss_fn, is_dense=False)\n",
    "        dense_total_loss += dense_losses.mean().item()\n",
    "        lstm_total_loss += lstm_losses.mean().item()\n",
    "\n",
    "    dense_total_loss /= 10\n",
    "    lstm_total_loss /= 10\n",
    "\n",
    "    results[f\"{distribution} synthetic\"] = {\n",
    "        'linear': linear_losses.mean().item(),\n",
    "        'dense': dense_total_loss,\n",
    "        'lstm': lstm_total_loss\n",
    "    }\n",
    "    synthetic_results[f\"{distribution} synthetic\"] = {\n",
    "        'linear': linear_losses.mean().item(),\n",
    "        'dense': dense_total_loss,\n",
    "        'lstm': lstm_total_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_results(synthetic_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_combined_pdfs('synthetic_1', dense_model, lstm_model, quant_reg, dense_normal_test_dataset, lstm_normal_test_dataset, is_linear=True)\n",
    "plot_pdf('synthetic_1', dense_model, lstm_model, quant_reg, dense_normal_test_dataset, lstm_normal_test_dataset, is_linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, wasserstein_distance, t, ks_2samp\n",
    "from typing import Dict, List, Union\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def wasserstein_metric(quantiles: np.ndarray, real_returns: np.ndarray) -> float:\n",
    "    \"\"\"Calculate Wasserstein distance for a single prediction window.\"\"\"\n",
    "    real_cdf = np.sort(real_returns)\n",
    "    pred_cdf = np.sort(quantiles)\n",
    "    return wasserstein_distance(real_cdf, pred_cdf)\n",
    "\n",
    "\n",
    "def ks_statistic(quantiles: np.ndarray, real_returns: np.ndarray) -> float:\n",
    "    \"\"\"Calculate KS statistic for a single prediction window.\"\"\"\n",
    "    return ks_2samp(quantiles, real_returns).statistic\n",
    "\n",
    "\n",
    "def quantile_coverage_error(quantiles: np.ndarray, real_returns: np.ndarray,\n",
    "                            quant_probs: np.ndarray) -> float:\n",
    "    \"\"\"Calculate QCE for a single prediction window.\"\"\"\n",
    "    empirical_proportions = np.array([\n",
    "        np.mean(real_returns <= q) for q in quantiles\n",
    "    ])\n",
    "    errors = np.abs(empirical_proportions - quant_probs)\n",
    "    return np.mean(errors)\n",
    "\n",
    "\n",
    "def historical_baseline(real_returns: np.ndarray, quant_probs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculate historical baseline quantiles.\"\"\"\n",
    "    return np.percentile(real_returns, quant_probs * 100)\n",
    "\n",
    "\n",
    "def gaussian_baseline(real_returns: np.ndarray, quant_probs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculate Gaussian baseline quantiles.\"\"\"\n",
    "    mu, sigma = np.mean(real_returns), np.std(real_returns)\n",
    "    return norm.ppf(quant_probs, loc=mu, scale=sigma)\n",
    "\n",
    "\n",
    "def student_t_baseline(real_returns: np.ndarray, quant_probs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculate Student's t baseline quantiles.\"\"\"\n",
    "    df, loc, scale = 5, np.mean(real_returns), np.std(real_returns)\n",
    "    return t.ppf(quant_probs, df, loc=loc, scale=scale)\n",
    "\n",
    "\n",
    "def evaluate_predictions_2d(predicted_quantiles: np.ndarray, future_returns: np.ndarray,\n",
    "                            observed_returns: np.ndarray, quant_probs: np.ndarray) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate distributional predictions across multiple time windows.\n",
    "\n",
    "    Args:\n",
    "        predicted_quantiles: Shape (n_windows, n_quantiles)\n",
    "        future_returns: Shape (n_windows, n_future_steps)\n",
    "        observed_returns: Shape (n_windows, n_observed_steps)\n",
    "        quant_probs: Shape (n_quantiles,)\n",
    "\n",
    "    Returns:\n",
    "        Dict containing mean and std of metrics across windows\n",
    "    \"\"\"\n",
    "    n_windows = predicted_quantiles.shape[0]\n",
    "    metrics_per_window = []\n",
    "\n",
    "    for i in range(n_windows):\n",
    "        # Generate baseline predictions for this window\n",
    "        hist_baseline = historical_baseline(observed_returns[i], quant_probs)\n",
    "        gauss_baseline = gaussian_baseline(observed_returns[i], quant_probs)\n",
    "        t_baseline = student_t_baseline(observed_returns[i], quant_probs)\n",
    "\n",
    "        # Calculate metrics for this window\n",
    "        window_metrics = {\n",
    "            'qce': {\n",
    "                'historical': quantile_coverage_error(hist_baseline, future_returns[i], quant_probs),\n",
    "                'gaussian': quantile_coverage_error(gauss_baseline, future_returns[i], quant_probs),\n",
    "                'student_t': quantile_coverage_error(t_baseline, future_returns[i], quant_probs),\n",
    "                'qlstm': quantile_coverage_error(predicted_quantiles[i], future_returns[i], quant_probs)\n",
    "            },\n",
    "            'wasserstein': {\n",
    "                'historical': wasserstein_metric(hist_baseline, future_returns[i]),\n",
    "                'gaussian': wasserstein_metric(gauss_baseline, future_returns[i]),\n",
    "                'student_t': wasserstein_metric(t_baseline, future_returns[i]),\n",
    "                'qlstm': wasserstein_metric(predicted_quantiles[i], future_returns[i])\n",
    "            },\n",
    "            'ks': {\n",
    "                'historical': ks_statistic(hist_baseline, future_returns[i]),\n",
    "                'gaussian': ks_statistic(gauss_baseline, future_returns[i]),\n",
    "                'student_t': ks_statistic(t_baseline, future_returns[i]),\n",
    "                'qlstm': ks_statistic(predicted_quantiles[i], future_returns[i])\n",
    "            }\n",
    "        }\n",
    "        metrics_per_window.append(window_metrics)\n",
    "\n",
    "    # Calculate aggregate statistics\n",
    "    aggregate_metrics = {\n",
    "        'mean': {},\n",
    "        'std': {},\n",
    "        'min': {},\n",
    "        'max': {},\n",
    "        'median': {}\n",
    "    }\n",
    "\n",
    "    # Helper function to extract specific metric across windows\n",
    "    def extract_metric(metrics_list, metric_path):\n",
    "        values = []\n",
    "        for m in metrics_list:\n",
    "            curr = m\n",
    "            for key in metric_path:\n",
    "                curr = curr[key]\n",
    "            values.append(curr)\n",
    "        return np.array(values)\n",
    "\n",
    "    # Metrics to aggregate\n",
    "    metric_paths = [\n",
    "        ('qce', model) for model in ['historical', 'gaussian', 'student_t', 'qlstm']\n",
    "    ] + [\n",
    "        ('wasserstein', model) for model in ['historical', 'gaussian', 'student_t', 'qlstm']\n",
    "    ] + [\n",
    "        ('ks', model) for model in ['historical', 'gaussian', 'student_t', 'qlstm']\n",
    "    ]\n",
    "\n",
    "    # Calculate statistics for each metric\n",
    "    for path in metric_paths:\n",
    "        values = extract_metric(metrics_per_window, path)\n",
    "        metric_name = '_'.join(path)\n",
    "\n",
    "        aggregate_metrics['mean'][metric_name] = np.mean(values)\n",
    "        aggregate_metrics['std'][metric_name] = np.std(values)\n",
    "        aggregate_metrics['min'][metric_name] = np.min(values)\n",
    "        aggregate_metrics['max'][metric_name] = np.max(values)\n",
    "        aggregate_metrics['median'][metric_name] = np.median(values)\n",
    "\n",
    "    return aggregate_metrics\n",
    "\n",
    "\n",
    "def print_aggregate_metrics(metrics: Dict):\n",
    "    \"\"\"Pretty print the aggregate metrics.\"\"\"\n",
    "    stats = ['mean', 'std', 'median', 'min', 'max']\n",
    "    metric_types = ['qce', 'wasserstein', 'ks']\n",
    "\n",
    "    for metric_type in metric_types:\n",
    "        print(f\"\\n{metric_type.upper()} Metrics:\")\n",
    "        relevant_metrics = {k: v for k, v in metrics['mean'].items() if metric_type in k}\n",
    "\n",
    "        for metric_name, mean_value in relevant_metrics.items():\n",
    "            print(f\"\\n{metric_name}:\")\n",
    "            for stat in stats:\n",
    "                print(f\"  {stat}: {metrics[stat][metric_name]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test_set = get_test_dataset(lstm_normalization_window, test_start_date, test_end_date, lookforward=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_market_group(\n",
    "    group_name: str,\n",
    "    prediction_test_set,\n",
    "    lstm_model,\n",
    "    device: str,\n",
    "    q: np.ndarray\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate predictions for a market group.\n",
    "    \n",
    "    Args:\n",
    "        group_name: Name of the market group\n",
    "        prediction_test_set: Dataset containing the test data\n",
    "        lstm_model: Trained LSTM model\n",
    "        device: Device to run computations on\n",
    "        q: Quantile probabilities\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing evaluation metrics\n",
    "    \"\"\"\n",
    "    group_assets = [a[\"asset\"] for a in prediction_test_set.datas[group_name]]\n",
    "    observed_returns = []\n",
    "    future_returns = []\n",
    "    all_pred_quantiles = []\n",
    "    \n",
    "    # Evaluate each asset in the group\n",
    "    for asset in group_assets:\n",
    "        prediction_test_set.set_main_asset(asset)\n",
    "        lstm_data_loader = DataLoader(prediction_test_set, batch_size=1024, shuffle=False)\n",
    "        lstm_model.eval()\n",
    "        \n",
    "        pred_quantiles = []\n",
    "        ys = []\n",
    "        \n",
    "        # Get predictions\n",
    "        for x, s, z, y, _ in lstm_data_loader:\n",
    "            x, s, z, y = x.to(device), s.to(device), z.to(device), y.to(device)\n",
    "            s = s.mean(dim=1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _, lstm_quantiles = lstm_model(x, s, z)\n",
    "            pred_quantiles.append(lstm_quantiles.detach().cpu().numpy())\n",
    "            ys.append(y.cpu().numpy())\n",
    "        \n",
    "        # Process predictions\n",
    "        pred_quantiles = np.concatenate(pred_quantiles, axis=0)/100\n",
    "        ys = np.concatenate(ys, axis=0)/100\n",
    "        ys = ys.reshape(ys.shape[0], -1)\n",
    "        \n",
    "        # Split data\n",
    "        sub_observed_returns = ys[:-30]\n",
    "        sub_future_returns = ys[30:]\n",
    "        sub_predicted_quantiles = pred_quantiles[30:]\n",
    "        \n",
    "        observed_returns.append(sub_observed_returns)\n",
    "        future_returns.append(sub_future_returns)\n",
    "        all_pred_quantiles.append(sub_predicted_quantiles)\n",
    "    \n",
    "    # Concatenate results\n",
    "    observed_returns = np.concatenate(observed_returns, axis=0)\n",
    "    future_returns = np.concatenate(future_returns, axis=0)\n",
    "    all_pred_quantiles = np.concatenate(all_pred_quantiles, axis=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    return evaluate_predictions_2d(\n",
    "        all_pred_quantiles, future_returns, observed_returns, q\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_markets(\n",
    "    market_groups: list[str],\n",
    "    prediction_test_set,\n",
    "    lstm_model,\n",
    "    device: str,\n",
    "    q: np.ndarray\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate and format results for all market groups.\n",
    "    \n",
    "    Returns:\n",
    "        Formatted DataFrame with comparisons across markets\n",
    "    \"\"\"\n",
    "    results_by_market = {}\n",
    "    \n",
    "    for group in market_groups:\n",
    "        # Get metrics for this market\n",
    "        metrics = evaluate_market_group(\n",
    "            group, prediction_test_set, lstm_model, device, q\n",
    "        )\n",
    "        \n",
    "        # Convert to DataFrame format\n",
    "        market_results = []\n",
    "        \n",
    "        # LSTM metrics\n",
    "        market_results.append({\n",
    "            'metric': 'qLSTM_wasserstein',\n",
    "            'mean': metrics['mean']['wasserstein_qlstm'],\n",
    "            'std': metrics['std']['wasserstein_qlstm']\n",
    "        })\n",
    "        market_results.append({\n",
    "            'metric': 'qLSTM_ks',\n",
    "            'mean': metrics['mean']['ks_qlstm'],\n",
    "            'std': metrics['std']['ks_qlstm']\n",
    "        })\n",
    "        market_results.append({\n",
    "            'metric': 'qLSTM_qce',\n",
    "            'mean': metrics['mean']['qce_qlstm'],\n",
    "            'std': metrics['std']['qce_qlstm']\n",
    "        })\n",
    "        \n",
    "        # Historical baseline metrics\n",
    "        market_results.append({\n",
    "            'metric': 'historical_wasserstein',\n",
    "            'mean': metrics['mean']['wasserstein_historical'],\n",
    "            'std': metrics['std']['wasserstein_historical']\n",
    "        })\n",
    "        market_results.append({\n",
    "            'metric': 'historical_ks',\n",
    "            'mean': metrics['mean']['ks_historical'],\n",
    "            'std': metrics['std']['ks_historical']\n",
    "        })\n",
    "        market_results.append({\n",
    "            'metric': 'historical_qce',\n",
    "            'mean': metrics['mean']['qce_historical'],\n",
    "            'std': metrics['std']['qce_historical']\n",
    "        })\n",
    "        \n",
    "        # Gaussian baseline metrics\n",
    "        market_results.append({\n",
    "            'metric': 'gaussian_wasserstein',\n",
    "            'mean': metrics['mean']['wasserstein_gaussian'],\n",
    "            'std': metrics['std']['wasserstein_gaussian']\n",
    "        })\n",
    "        market_results.append({\n",
    "            'metric': 'gaussian_ks',\n",
    "            'mean': metrics['mean']['ks_gaussian'],\n",
    "            'std': metrics['std']['ks_gaussian']\n",
    "        })\n",
    "        market_results.append({\n",
    "            'metric': 'gaussian_qce',\n",
    "            'mean': metrics['mean']['qce_gaussian'],\n",
    "            'std': metrics['std']['qce_gaussian']\n",
    "        })\n",
    "        \n",
    "        # Student-t baseline metrics\n",
    "        market_results.append({\n",
    "            'metric': 'student_t_wasserstein',\n",
    "            'mean': metrics['mean']['wasserstein_student_t'],\n",
    "            'std': metrics['std']['wasserstein_student_t']\n",
    "        })\n",
    "        market_results.append({\n",
    "            'metric': 'student_t_ks',\n",
    "            'mean': metrics['mean']['ks_student_t'],\n",
    "            'std': metrics['std']['ks_student_t']\n",
    "        })\n",
    "        market_results.append({\n",
    "            'metric': 'student_t_qce',\n",
    "            'mean': metrics['mean']['qce_student_t'],\n",
    "            'std': metrics['std']['qce_student_t']\n",
    "        })\n",
    "        \n",
    "        results_by_market[group] = pd.DataFrame(market_results)\n",
    "    \n",
    "    # Format results into pretty table\n",
    "    return results_by_market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_groups = [\n",
    "    \"nikkei 225\", \n",
    "    \"s&p 500\", \n",
    "    \"euro stoxx 50\", \n",
    "    \"cryptocurrencies\",\n",
    "    \"commodities\",\n",
    "    \"currency pairs\"\n",
    "]\n",
    "\n",
    "results_df = evaluate_all_markets(\n",
    "    market_groups,\n",
    "    prediction_test_set,\n",
    "    lstm_model,\n",
    "    DEVICE,\n",
    "    np.array(quantiles)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metric_tables(data):\n",
    "    \"\"\"\n",
    "    Generate comparison tables for wasserstein, ks, and qce metrics across different methods and markets,\n",
    "    with an added 'total' row for each metric.\n",
    "\n",
    "    Args:\n",
    "    - data (dict): Dictionary where keys are market names and values are DataFrames with columns 'metric', 'mean', and 'std'.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary of DataFrames for each metric (wasserstein, ks, qce).\n",
    "    \"\"\"\n",
    "    metrics = ['wasserstein', 'ks', 'qce']\n",
    "    tables = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        # Initialize the structure of the metric table\n",
    "        metric_data = {\n",
    "            'market': [],\n",
    "            'qLSTM': [],\n",
    "            'historical': [],\n",
    "            'gaussian': [],\n",
    "            'student_t': []\n",
    "        }\n",
    "\n",
    "        # Populate the table for the current metric\n",
    "        for market, df in data.items():\n",
    "            metric_data['market'].append(market)\n",
    "            for method in ['qLSTM', 'historical', 'gaussian', 'student_t']:\n",
    "                value = df.loc[df['metric'] == f'{method}_{metric}', 'mean'].values[0]\n",
    "                metric_data[method].append(value)\n",
    "\n",
    "        # Create a DataFrame for the current metric\n",
    "        metric_df = pd.DataFrame(metric_data)\n",
    "\n",
    "        # Calculate totals (mean of columns) and append as the last row\n",
    "        total_row = {'market': 'total'}\n",
    "        for method in ['qLSTM', 'historical', 'gaussian', 'student_t']:\n",
    "            total_row[method] = metric_df[method].mean()\n",
    "        metric_df = pd.concat([metric_df, pd.DataFrame([total_row])], ignore_index=True)\n",
    "\n",
    "        # Store the table\n",
    "        tables[metric] = metric_df\n",
    "\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_tables = generate_metric_tables(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (metric, table) in enumerate(metrics_tables.items()):\n",
    "    print(f\"\\n{metric.upper()} Metrics:\")\n",
    "    display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
